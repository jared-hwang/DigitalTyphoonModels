Start time: 2023_04_28-16.32.29 
Num epochs: 20 
Batch size: 16 
Learning rate: 0.1 
 Split by: year 
Epoch 1 
 	 loss: 0.9410281555765795 
 	 acc: 59.472044442341875 
Epoch 2 
 	 loss: 0.7164619312054481 
 	 acc: 68.88900903579852 
Epoch 3 
 	 loss: 0.6455724987694446 
 	 acc: 72.10526671487561 
Epoch 4 
 	 loss: 0.5790237746174064 
 	 acc: 75.3755905032879 
Epoch 5 
 	 loss: 0.5069108496566712 
 	 acc: 78.84122811167354 
Epoch 6 
 	 loss: 0.4264027969919751 
 	 acc: 82.4933937972656 
Epoch 7 
 	 loss: 0.3464545843835556 
 	 acc: 86.18475741212568 
Epoch 8 
 	 loss: 0.2671541256822443 
 	 acc: 89.66323572147844 
Epoch 9 
 	 loss: 0.20185382558080403 
 	 acc: 92.34221143903709 
Epoch 10 
 	 loss: 0.14796853715100303 
 	 acc: 94.48458102144397 
Epoch 11 
 	 loss: 0.11151324973362002 
 	 acc: 95.85921185129116 
Epoch 12 
 	 loss: 0.08561078820841563 
 	 acc: 96.9182317678942 
Epoch 13 
 	 loss: 0.06652545320271028 
 	 acc: 97.65961329215298 
Epoch 14 
 	 loss: 0.0549121087145848 
 	 acc: 98.08200477133416 
Epoch 15 
 	 loss: 0.04405382121147429 
 	 acc: 98.4455993566133 
Epoch 16 
 	 loss: 0.038041643629738366 
 	 acc: 98.68957267498834 
Epoch 17 
 	 loss: 0.030375174119334845 
 	 acc: 98.95652408983085 
Epoch 18 
 	 loss: 0.022780559252313707 
 	 acc: 99.24172281657397 
Epoch 19 
 	 loss: 0.024482273497426474 
 	 acc: 99.18562922813871 
Epoch 20 
 	 loss: 0.020617707772892575 
 	 acc: 99.30727797414288 
Validation: 
 	 loss: 20.570964492012052 
 	 acc: 49.51619177282895 
 	 micro_f1: 0.49516191772828944 
 	 macro_f1: 0.4691653529463511
 	 weighted_f1: 0.48290344607443586